# -*- coding: utf-8 -*-
"""machlear.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndbpNTOp12pRjnYs3dcF_mw707lEPetw

#Import liberaries
"""

import numpy as np
import pandas as pd
from statistics import mean
import matplotlib.pyplot as plt

"""#Import and Mount drive"""

from google.colab import drive
drive.mount('/content/drive')
data=pd.read_csv('/content/drive/MyDrive/Dsata Set for Assignment 1.csv')

"""# Data Manipulations """

#dropping id
data=data.drop(['id'],axis=1)
data=data.dropna()
data=data.replace(to_replace='M',value=1)
data=data.replace(to_replace='B',value=0)

"""# Creating a class Perceptron"""

from numpy.core.multiarray import nested_iters
class Perceptron:
    def __init__(self,learning_rate=0.01,n_iters=2000):
        self.lr=learning_rate
        self.n_iters=n_iters
        self.activation_func=self._unit_step_func
        self.weights=[]
        self.bias=0
    #fit function will take training data and adjust weights and bias
    def fit(self,X,Y):
        n_samples,n_features=X.shape
        self.weights=np.zeros(n_features)
        self.bias=0

        for num in range(self.n_iters):
            bias_=0
            for i in range(len(X)):
                y_out=np.dot(X[i],self.weights)+self.bias
                y_predicted=self.activation_func(y_out)

                updated=self.lr*(Y[i]-y_predicted)
                self.weights +=updated*X[i]
                bias_ += int(updated !=0.0)
        return self.weights

    #predict function will take test examples and return the target 
    #attribute(y=0 or y=1)
    def predict(self,X,res_weights):
        linear_opt=np.dot(X,res_weights)+self.bias
        y_predicted=self.activation_func(linear_opt)
        return y_predicted

    #convert y_predicted using step function
    def _unit_step_func(self,x):
        return np.where(x>=0,1,0)

    #accuracy function compare all pair of (y_true and y_predicted)
    #finds sum=matching pairs
    def accuracy(self,y_true,y_pred):
        sum=0
        for idx in range(len(y_true)):
            if y_true[idx]==y_pred[idx]:
                sum+=1
        return (sum*(100))/len(y_true)

"""#create training data and testing data

#Here Random state=100
#Random state in (axis=1) defines order of training examples
"""
result1=[]
result2=[]
result3=[]
num_splits=10
for i in range(num_splits):
    randx=100+5*i
    train=data.sample(frac=0.67,random_state=randx)
    test = data.drop(train.index)
    X_train,Y_train=train.drop('diagnosis',axis=1),train['diagnosis']
    X_test,Y_test=test.drop('diagnosis',axis=1),test['diagnosis']
    p1=Perceptron()
    res_weights=p1.fit(X_train.to_numpy(),Y_train.to_numpy())

    """#Finding accuracy"""

    y_pred=np.zeros(len(X_test))
    for idx in range(len(X_test)):
        y_pred[idx]=p1.predict(X_test.to_numpy()[idx],res_weights)
    result1.append(p1.accuracy(Y_test.to_numpy(),y_pred))

    """#Creating Model PM3-NORMALIZING DATA"""
    data1=data.drop('diagnosis',axis=1)
    variance=data1.var().to_numpy()
    mean=data1.mean().to_numpy()
    narray=X_train.to_numpy()
    nrows,ncolumns=narray.shape
    #NORMALIZE DATA
    for i in range(nrows):
        for j in range(ncolumns):
            narray[i][j]=(narray[i][j]-mean[j])/variance[j]
    n2array=X_test.to_numpy()
    n2rows,n2columns=n2array.shape
    for i in range(n2rows):
        for j in range(n2columns):
            n2array[i][j]=(n2array[i][j]-mean[j])/variance[j]
    p2=Perceptron()
    new_weights=p2.fit(narray,Y_train.to_numpy())
    y_newpred=np.zeros(len(X_test))
    y_newpred2=np.zeros(len(X_train))
    for idx in range(len(X_test)):
        y_newpred[idx]=p2.predict(n2array[idx],new_weights)
    for idx in range(len(X_train)):
        y_newpred2[idx]=p2.predict(narray[idx],new_weights)

    print("Accuracy of testing examples :",p2.accuracy(Y_test.to_numpy(),y_newpred))
    print("Accuracy of training examples :",p2.accuracy(Y_train.to_numpy(),y_newpred2))
    result2.append(p2.accuracy(Y_train.to_numpy(),y_newpred2))

"""#PM4"""

splits=10
for i in range(splits):
    rand1=100+5*i
    rand2=200+5*i
    temp_data=data.sample(frac=1,random_state=rand1,axis=1)
    train=temp_data.sample(frac=0.67,random_state=rand2)
    test = temp_data.drop(train.index)
    X_train,Y_train=train.drop('diagnosis',axis=1),train['diagnosis']
    X_test,Y_test=test.drop('diagnosis',axis=1),test['diagnosis']
    p1=Perceptron()
    res_weights=p1.fit(X_train.to_numpy(),Y_train.to_numpy())
    y_pred=np.zeros(len(X_test))
    for idx in range(len(X_test)):
        y_pred[idx]=p1.predict(X_test.to_numpy()[idx],res_weights)
    result3.append(p1.accuracy(Y_test.to_numpy(),y_pred))
print("Mean accuracy for task1 :",np.mean(result1))
print("Variance for task1 :",np.var(result1))
print("Mean accuracy for task2 :",np.mean(result2))
print("Variance for task2 :",np.var(result2))
print("Mean accuracy for task3 :",np.mean(result3))
print("Variance for task3 :",np.var(result3))